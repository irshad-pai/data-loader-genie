version: "3.7"
services:

#  hive-metastore:
#    image: 'bitsondatadev/hive-metastore:latest'
#    container_name: hive-metastore
#    hostname: hive-metastore
#    ports:
#      - '9083:9083' # Metastore Thrift
#    volumes:
#      - ./config/metastore-site.xml:/opt/apache-hive-metastore-3.0.0-bin/conf/metastore-site.xml
#    environment:
#      - METASTORE_DB_HOSTNAME=mariadb
#      - AWS_ACCESS_KEY_ID=minioadmin
#      - AWS_SECRET_ACCESS_KEY=minioadmin
#    depends_on:
#      - mariadb
#      - minio
#    networks:
#      - dbt_sample_network

#  spark-thrift:
#    image: spark-thrift
#    container_name: spark-thrift
#    build:
#      context: .
#      dockerfile: SparkThriftDockerfile
#    ports:
#      - "10000:10000"
#      - "4040:4040"
#    depends_on:
#      - hive-metastore
#      - createbuckets
#    command: >
#      --class org.apache.spark.sql.hive.thriftserver.HiveThriftServer2
#      --name Thrift JDBC/ODBC Server
#      --hiveconf hive.server2.thrift.bind.host=0.0.0.0
#    volumes:
#      - ./.spark-warehouse/:/spark-warehouse/
#      - ./config/hive-site.xml:/usr/spark/conf/hive-site.xml
#      - ./config/spark-defaults.conf:/usr/spark/conf/spark-defaults.conf
#      - ./sds:/sds
#    environment:
#      - WAIT_FOR=hive-metastore:9083
#      - AWS_ACCESS_KEY_ID=minioadmin
#      - AWS_SECRET_ACCESS_KEY=minioadmin
#      - HIVE_SERVER2_THRIFT_BIND_HOST=spark-thrift
#    networks:
#      - dbt_sample_network

#  mariadb:
#    image: 'mariadb:latest'
#    hostname: mariadb
#    container_name: mariadb
#    ports:
#      - '3306:3306'
#    environment:
#      MYSQL_ROOT_PASSWORD: admin
#      MYSQL_USER: admin
#      MYSQL_PASSWORD: admin
#      MYSQL_DATABASE: metastore_db
#    volumes:
#      - ./.hive-metastore:/var/lib/mysql
#      - ./sds:/sds
#    networks:
#      - dbt_sample_network

#  minio:
#    image: 'minio/minio:latest'
#    hostname: minio
#    container_name: minio
#    ports:
#      - '9000:9000'
#      - '9001:9001'
#    volumes:
#      - .minio-data:/data
#    environment:
#      MINIO_ACCESS_KEY: minioadmin
#      MINIO_SECRET_KEY: minioadmin
#      MINIO_API_SELECT_PARQUET: "on"
#    command: server --console-address ":9001" /data
#    networks:
#      - dbt_sample_network

#  createbuckets:
#    image: minio/mc
#    depends_on:
#      - minio
#    entrypoint: >
#      /bin/sh -c "
#      /usr/bin/mc config host add myminio http://minio:9000 minioadmin minioadmin;
#      /usr/bin/mc mb myminio/warehouse;
#      /usr/bin/mc policy download myminio/warehouse;
#      /usr/bin/mc mb myminio/sds;
#      /usr/bin/mc policy download myminio/sds;
#      /usr/bin/mc cp /sds/minio/active-directory.json myminio/sds/raw/active-directory.json;
#      exit 0;
#      "
#    volumes:
#      - ./sds:/sds
#    networks:
#      - dbt_sample_network
#  nifi:
#    image: apache/nifi:latest
#    container_name: nifi
#    hostname: nifi
#    ports:
#      - '8080:8080' # Unsecured HTTP Web Port
#    environment:
#      - NIFI_WEB_HTTP_PORT=8080
#    networks:
#      - dbt_sample_network
#    volumes:
#      - ./sds:/sds

  redis:
    image: redis
    hostname: airflow_redis
    container_name: airflow_redis
    ports:
      - "6379:6379"
    networks:
      - dbt_sample_network

  postgres:
    image: postgres:9.6
    env_file:
      - development.env
    ports:
      - "5433:5432"
    logging:
      options:
        max-size: 10m
        max-file: "3"
    volumes:
      - db-data:/var/lib/postgresql/data
    networks:
      - dbt_sample_network

  webserver:
    image: airflow-local
    build:
      context: .
      dockerfile: AirflowDockerfile
    container_name: airflow_webserver
    hostname: airflow_webserver
    restart: always
    depends_on:
      - postgres
    env_file:
      - development.env
    ports:
      - "7200:8080"
    command: bash -c """airflow db init && airflow webserver """
    networks:
      - dbt_sample_network
    volumes:
      - ./sds/airflow/dags:/opt/airflow/dags
      - ./webserver_config.py:/opt/airflow/webserver_config.py
      - ./airflow.cfg:/opt/airflow/airflow.cfg
    extra_hosts:
      - "host.docker.internal:host-gateway"

  scheduler:
    image: airflow-local
    container_name: airflow_scheduler
    build:
      context: .
      dockerfile: AirflowDockerfile
    hostname: airflow_scheduler
    entrypoint: /bin/bash
    command:
      - -c
      - airflow scheduler
    depends_on:
      - postgres
      - webserver
    env_file:
      - development.env
    volumes:
      - ./sds/airflow/dags:/opt/airflow/dags
      - ./webserver_config.py:/opt/airflow/webserver_config.py
      - ./airflow.cfg:/opt/airflow/airflow.cfg
    restart: always
    extra_hosts:
      - "host.docker.internal:host-gateway"
    networks:
      - dbt_sample_network

  worker:
    image: airflow-local
    container_name: airflow_worker
    hostname: airflow_worker
    build:
      context: .
      dockerfile: AirflowDockerfile
    entrypoint: /bin/bash
    command:
      - -c
      - airflow celery worker
    depends_on:
      - postgres
      - webserver
    env_file:
      - development.env
    volumes:
      - ./webserver_config.py:/opt/airflow/webserver_config.py
      - ./airflow.cfg:/opt/airflow/airflow.cfg
      - ./sds/airflow/dags:/opt/airflow/dags
    restart: always
    extra_hosts:
      - "host.docker.internal:host-gateway"
    networks:
      - dbt_sample_network

networks:
  dbt_sample_network:
    driver: bridge
volumes:
    db-data:
        driver: local
